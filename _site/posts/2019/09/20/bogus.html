<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Your p-values Are Bogus - Ramya Bygari</title>
<meta name="description" content="Is hypothesis testing built upon a house of lies? No, probably not. But still, read this article.">


  <meta name="author" content="Ramya Bygari">
  
  <meta property="article:author" content="Ramya Bygari">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Ramya Bygari">
<meta property="og:title" content="Your p-values Are Bogus">
<meta property="og:url" content="http://localhost:4000/posts/2019/09/20/bogus.html">


  <meta property="og:description" content="Is hypothesis testing built upon a house of lies? No, probably not. But still, read this article.">







  <meta property="article:published_time" content="2019-09-20T00:00:00+05:30">






<link rel="canonical" href="http://localhost:4000/posts/2019/09/20/bogus.html">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Ramya Bygari Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Ramya Bygari
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/">Home</a>
            </li><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/portfolio/">Portfolio</a>
            </li><li class="masthead__menu-item">
              <a href="/experience/">Experience</a>
            </li><li class="masthead__menu-item">
              <a href="/assets/docs/resume.pdf">Resume</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/Ramya.png" alt="Ramya Bygari" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">Ramya Bygari</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Machine Learning and Scalable Systems</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">Bangalore, India</span>
        </li>
      

      

      

      
        <li>
          <a href="mailto:ramyabygari239@gmail.com" rel="me" class="u-email">
            <meta itemprop="email" content="ramyabygari239@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      
        <li>
          <a href="https://www.linkedin.com/in/ramyabygari" itemprop="sameAs" rel="nofollow noopener noreferrer me">
            <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span>
          </a>
        </li>
      

      

      

      

      

      
        <li>
          <a href="https://github.com/ramyabygari" itemprop="sameAs" rel="nofollow noopener noreferrer me">
            <i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Your p-values Are Bogus">
    <meta itemprop="description" content="Is hypothesis testing built upon a house of lies? No, probably not. But still, read this article.">
    <meta itemprop="datePublished" content="2019-09-20T00:00:00+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/posts/2019/09/20/bogus.html" class="u-url" itemprop="url">Your p-values Are Bogus
</a>
          </h1>
          


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right ">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> Contents</h4></header>
              <ul class="toc__menu"><li><a href="#a-simple-example">A Simple Example</a><ul><li><a href="#python-simulation-of-data">Python Simulation of Data</a></li><li><a href="#convergence-is-not-enough">Convergence Is Not Enough</a></li><li><a href="#implications">Implications</a></li></ul></li><li><a href="#afterword-technical-details">Afterword: Technical Details</a><ul><li><a href="#limitations-of-the-central-limit-theorem">Limitations of the Central Limit Theorem</a></li><li><a href="#the-berry-esseen-theorem">The Berry-Esseen Theorem</a></li></ul></li></ul>

            </nav>
          </aside>
        
        <p>People often use a Gaussian to approximate distributions of sample means. This is
generally justified by the central limit theorem, which states that the sample mean of
an independent and identically distributed sequence of random variables converges to a
normal random variable in distribution.<sup id="fnref:fnote_clt"><a href="#fn:fnote_clt" class="footnote">1</a></sup> In hypothesis testing, we might use
this to calculate a <script type="math/tex">p</script>-value, which then is used to drive decision making.</p>

<p>I’m going to show that calculating <script type="math/tex">p</script>-values in this way is actually incorrect, and
leads to results that get <em>less</em> accurate as you collect more data! This has
substantial implications for those who care about the statistical rigor of their A/B
tests, which are often based on Gaussian (normal) approximations.</p>

<h1 id="a-simple-example">A Simple Example</h1>

<p>Let’s take a very simple example. Let’s say that the prevailing wisdom is that no more
than 20% of people like rollerskating. You suspect that the number is in fact much
larger, and so you decide to run a statistical test. In this test, you model each person
as a Bernoulli random variable with parameter <script type="math/tex">p</script>. <strong>The null hypothesis <script type="math/tex">H_0</script> is
that <script type="math/tex">p\leq 0.2</script></strong>. You decide to go out and ask 100 people their opinions on
rollerskating.<sup id="fnref:fnote_sample"><a href="#fn:fnote_sample" class="footnote">2</a></sup></p>

<p>You begin gathering data. Unbeknownst to you, it is <em>in fact</em> the case that a full 80%
of the population enjoys rollerskating. So, as you randomly ask people if they enjoy
rollerskating, you end up getting a lot of “yes” responses. Once you’ve gotten 100
responses, you start analyzing the data.</p>

<p>It turns out that you got 74 “yes” responses, and 26 “no” responses. Since you’re a
practiced statistician, you know that you can calculate a <script type="math/tex">p</script>-value by finding the
probability that a binomial random variable with parameter <script type="math/tex">p_0=0.2</script> would generate a
value <script type="math/tex">k\geq74</script> with <script type="math/tex">n=100</script>. This probability is just</p>

<script type="math/tex; mode=display">p_\text{exact} = \text{Prob}(k\geq 74) = \sum_{k=74}^{n}{n \choose k} p_0^{k} (1-p_0)^{(n-k)}.</script>

<p>However, you know that you can approximate a binomial distribution with a Gaussian of
mean <script type="math/tex">\mu=np_0</script> and variance <script type="math/tex">\sigma^2=np_0(1-p_0)</script>, so you decide to calculate an
<em>approximate</em> <script type="math/tex">p</script>-value,</p>

<script type="math/tex; mode=display">p_\text{approx} = \frac{1}{\sqrt{2\pi np_0(1-p_0)}}\int_{k=74}^\infty \exp\left(-\frac{(k-np_0)^2}{2np_0(1-p_0)}\right).</script>

<p>However, <strong>this approximation is actually incorrect, and will give you progressively
worse estimates of <script type="math/tex">p_\text{exact}</script>.</strong> Let’s observe this in action.</p>

<h2 id="python-simulation-of-data">Python Simulation of Data</h2>

<p>We simulate data for values <script type="math/tex">n=1</script> through <script type="math/tex">n=1000</script>, and compute the corresponding
exact and approximate <script type="math/tex">p</script>-value. We plot the log of the <script type="math/tex">p</script> value, since they get
very small very quickly.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span><span class="p">,</span> <span class="n">binom</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span> <span class="p">[</span><span class="s">'classic'</span><span class="p">,</span> <span class="s">'ggplot'</span><span class="p">])</span>

<span class="n">p_true</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p_true</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">p0</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">p_vals</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">index</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">),</span> 
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'true p-value'</span><span class="p">,</span> <span class="s">'normal approx. p-value'</span><span class="p">]</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">n0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">):</span>
    <span class="n">normal_dev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n0</span><span class="o">*</span><span class="n">p0</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p0</span><span class="p">))</span>
    <span class="n">normal_mean</span> <span class="o">=</span> <span class="n">n0</span><span class="o">*</span><span class="n">p0</span>
    <span class="n">k</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="n">n0</span><span class="p">])</span>
    <span class="c"># the "survival function" is 1 - cdf, which is the p-value in our case</span>
    <span class="n">normal_logpval</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">logsf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">normal_mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">normal_dev</span><span class="p">)</span>
    <span class="n">true_logpval</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">logsf</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p0</span><span class="p">)</span>
    <span class="n">p_vals</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">n0</span><span class="p">,</span> <span class="s">'true p-value'</span><span class="p">]</span> <span class="o">=</span> <span class="n">true_logpval</span>
    <span class="n">p_vals</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">n0</span><span class="p">,</span> <span class="s">'normal approx. p-value'</span><span class="p">]</span> <span class="o">=</span> <span class="n">normal_logpval</span>
    
<span class="n">p_vals</span><span class="o">.</span><span class="n">replace</span><span class="p">([</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">));</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Number of Samples"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Log-p Value"</span><span class="p">);</span>
</code></pre></div></div>

<p>We have to drop <code class="highlighter-rouge">inf</code>s because after about <script type="math/tex">n=850</script> or so, the <script type="math/tex">p</script>-value actually
gets too small for <code class="highlighter-rouge">scipy.stats</code> to calculate; it just returns <code class="highlighter-rouge">-np.inf</code>.</p>

<p>The resulting plot tells a shocking tale:</p>

<p><img src="/assets/images/p-values.png" alt="P-value Divergence" /></p>

<p>The approximation diverges from the exact value! Seeing this, you begin to weep
bitterly. Is the Central Limit Theorem invalid? Has your whole life been a lie? It turns
out that the answer to the first is a resounding no, and the second… probably also
no. But then what is going on here?</p>

<h2 id="convergence-is-not-enough">Convergence Is Not Enough</h2>

<p>The first thing to note is that, mathematically speaking, the two <script type="math/tex">p</script>-values
<script type="math/tex">p_\text{exact}</script> and <script type="math/tex">p_\text{approx}</script> <strong>do, in fact, converge</strong>. That is to say,
as we increase the number of samples, their difference is approaching zero:</p>

<script type="math/tex; mode=display">\left| p_\text{exact} - p_\text{approx}\right| \rightarrow 0</script>

<p>What I’m arguing, then, is that <strong>convergence is not enough</strong>.</p>

<p>If it were, then we could just approximate the true <script type="math/tex">p</script>-value with 0. That is, we
could report a <script type="math/tex">p</script>-value of <script type="math/tex">p_\text{approx} = 0</script>, and claim that since our
approximation is converging to the actual value, it should be taken
seriously. Obviously, this should not be taken seriously as an approximation.</p>

<p>Our intuitive sense of “convergence”, the sense that <script type="math/tex">p_\text{approx}</script> is becoming “a
better and better approximation of” <script type="math/tex">p_\text{exact}</script> as we take more samples,
corresponds to the <em>percent error</em> converging to zero:</p>

<script type="math/tex; mode=display">\left| \frac{p_\text{approx} - p_\text{exact}}{p_\text{exact}}\right| \rightarrow 0.</script>

<p>In terms of asymptotic decay, this is a stronger claim than convergence. Rather than
their difference converging to zero, which means it is <script type="math/tex">o(1)</script>, we demand that their
difference converge to zero <em>faster than <script type="math/tex">p_\text{exact}</script></em>,</p>

<script type="math/tex; mode=display">\left| p_\text{exact} - p_\text{approx}\right|  = o\left(p_\text{exact}\right).</script>

<p>It would also suffice to have an upper bound on the <script type="math/tex">p</script>-value; that is, if we could
say that <script type="math/tex">% <![CDATA[
p_\text{exact} < p_\text{approx} %]]></script>, so <script type="math/tex">p_\text{exact}</script> is <em>at worst</em> our
approximate value <script type="math/tex">p_\text{approx}</script>, and we knew that this held regardless of sample
size, then we could report our approximate result knowing that it was at worst a bit
conservative. However, as far as I can see, the central limit theorem and other similar
convergence results give us no such guarantee.</p>

<h2 id="implications">Implications</h2>

<p>What I’ve shown is that for the simple case above, Gaussian approximation is not a
strategy that will get you good estimates of the true <script type="math/tex">p</script>-value, especially for large
amounts of data. You will under-estimate your <script type="math/tex">p</script>-value, and therefore overestimate
the strength of evidence you have against the null hypothesis.</p>

<p>Although A/B testing is a slightly more complex scenario, I suspect that the same
problem exists in that realm. A refresher on a typical A/B test scenario: you, as the
administrator of the test, care about the difference between two sample means. If they
samples are from Bernoulli random variables (a good model of click-through rates), then
the <em>true</em> distribution of this difference is the distribution of the difference of
(scaled) binomial random variables, which is more difficult to write down and work
with. Of course, the Gaussian approximation is simple, since the difference of two
Gaussians is again a Gaussian.<sup id="fnref:fnote_AB"><a href="#fn:fnote_AB" class="footnote">3</a></sup></p>

<p>Most statistical tests are approximate in this way. For example, the <script type="math/tex">\chi^2</script> test for
goodness of fit is an approximate test. So what are we to make of the fact that this
approximation does not guarantee increasingly valid <script type="math/tex">p</script>-values? Honestly, I don’t
know. I’m sure that others have considered this issue, but I’m not familiar with the
thinking of the statistical community on it. (As always, please comment if you know
something that would help me understand this better.) All I know is that when doing
tests like this in the future, I’ll be much more careful about how I report my results.</p>

<h1 id="afterword-technical-details">Afterword: Technical Details</h1>

<p>As I said above, the two <script type="math/tex">p</script>-values do, in fact, converge. However, there is an
interesting mathematical twist in that <strong>the convergence is not guaranteed by the
central limit theorem.</strong> It’s a bit besides the point, and quite technical, but I found
it so interesting that I thought I should write it up.</p>

<p>As I said, this section isn’t essential to my central argument about the insufficiency
of simple convergence; it’s more of an interesting aside.</p>

<h2 id="limitations-of-the-central-limit-theorem">Limitations of the Central Limit Theorem</h2>

<p>To understand the problem, we have to do a deep dive into the details of the central
limit theorem. This will get technical. The TL;DR is that since our <script type="math/tex">p</script>-values are
getting smaller, the CLT doesn’t actually guarantee that they will converge.</p>

<p>Suppose we have a sequence of random variables <script type="math/tex">X_1, X_2, X_3, \ldots</script>. These would
be, in the example above, the Bernoulli random variables that represent individual people’s
responses to your question about rollerskates. Suppose that these random variables are
independent and identically distributed, with mean <script type="math/tex">\mu</script> and finite variance
<script type="math/tex">\sigma^2</script>.<sup id="fnref:fnote_bin"><a href="#fn:fnote_bin" class="footnote">4</a></sup></p>

<p>Let <script type="math/tex">S_n</script> be the sample mean of all the <script type="math/tex">X_i</script> up through <script type="math/tex">n</script>:</p>

<script type="math/tex; mode=display">S_n = \frac{1}{n} \sum_{i=1}^n X_i.</script>

<p>We want to say what distribution the sample mean converges to. First, we know it’ll
converge to something close to the mean, so let’s subtract that off so that it converges
to something close to zero. So now we’re considering <script type="math/tex">S_n - \mu</script>. But we also know
that the standard deviation goes down like <script type="math/tex">1/\sqrt{n}</script>, so to get it to converge to
something stable, we have to multiply by <script type="math/tex">\sqrt{n}</script>. So now we’re considering the
shifted and scaled sample mean <script type="math/tex">\sqrt{n}\left(S_n - \mu\right)</script>.</p>

<p>The central limit theorem states that this converges <strong>in distribution</strong> to a normal
random variable with distribution <script type="math/tex">N(0, \sigma^2)</script>. Notationally, you might see
mathematicians write</p>

<script type="math/tex; mode=display">\sqrt{n}\left(S_n-\mu\right)\ \xrightarrow{D} N(0,\sigma^2).</script>

<p>What does it mean that they converge <strong>in distribution</strong>? It means that, for a fixed
area, the areas under the respective curves converge. Note that <strong>we have to fix the
area</strong> to get convergence. Let’s look at some pictures. First, note that we can plot the
exact distribution of the variable <script type="math/tex">\sqrt{n}(S_n-\mu)</script>; it’s just a binomial random
variable, appropriately shifted and scaled. We’ll plot this alongside the normal
approximation <script type="math/tex">N(0,\sigma^2)</script>.</p>

<!-- I'd like to have this centered. -->
<p><img src="/assets/images/clt.gif" alt="CLT gif" /></p>

<p>The area under the shaded part of the normal converges to the area of the bars in that
same shaded region. This is what convergence in distribution means.</p>

<p>Now for the crux. As we gather data, it becomes more and more obvious that our null
hypothesis is incorrect - that is, we move further and further out into the tail of the
null hypothesis’ distribution for <script type="math/tex">S_n</script>. This is very intuitive - as we gather more
data, we expect our <script type="math/tex">p</script>-value to go down. The <script type="math/tex">p</script>-value is a tail integral of the
distribution, so we expect to be moving further and further into the tail of the
distribution.</p>

<p>Here’s a gif, where the shaded region represents the <script type="math/tex">p</script>-value that we’re calculating:</p>

<!-- I'd like to have this centered. -->
<p><img src="/assets/images/p-val.gif" alt="p-value gif" /></p>

<p>As we increase <script type="math/tex">n</script>, the area we’re integrating changes. So we don’t get convergence
guarantees from the CLT.</p>

<h2 id="the-berry-esseen-theorem">The Berry-Esseen Theorem</h2>

<p>It’s worth noting that there is a stronger statement of convergence that applies
specifically to the convergence of the binomial distribution to the corresponding
Gaussian. It is called the <strong>Barry-Esseen Theorem</strong>, and it states that the maximum
distance between the cumulative probability functions of the binomial and the
corresponding Gaussian is <script type="math/tex">o(n^{-1/2})</script>. This claim, which is akin to uniform
convergence of functions (compare to the pointwise convergence of the CLT) does, in
fact, guarantee that our <script type="math/tex">p</script>-values will converge.</p>

<p>But, as I’ve said above, this is immaterial, albeit interesting; we know already that
the <script type="math/tex">p</script>-values converge, and we also know that this is not enough for us to be
reporting one as an approximation of the other.</p>

<!-------------------------------- FOOTER ---------------------------->

<div class="footnotes">
  <ol>
    <li id="fn:fnote_clt">
      <p>So long as the variance of the distribution being sampled is finite. <a href="#fnref:fnote_clt" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:fnote_sample">
      <p>You should decide this number based on some alternative hypothesis and
a power analysis. Also, you should ensure that you are sampling people evenly -
going to a park, for example, might bias your sample towards those that enjoy
rollerskating. <a href="#fnref:fnote_sample" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:fnote_AB">
      <p>I haven’t done a numerical test on this scenario because the true
distribution (the difference between two scaled binomials) is nontrivial to
calcualte, and numerical issues arise as we calculate such small <script type="math/tex">p</script>-values, which
SciPy takes care of for us in the above example. But as I said, I would be
unsurprised if our Gaussian-approximated <script type="math/tex">p</script>-values are increasingly poor
approximations of the true <script type="math/tex">p</script>-value as we gather more samples. <a href="#fnref:fnote_AB" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:fnote_bin">
      <p>In our case, for a single Bernoulli random variable with parameter <script type="math/tex">p</script>,
we have <script type="math/tex">\mu=p</script> and <script type="math/tex">\sigma^2=p(1-p)</script>. <a href="#fnref:fnote_bin" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

        
      </section>

      <footer class="page__meta">
        
        


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2019-09-20T00:00:00+05:30">September 20, 2019</time></p>

      </footer>

      

      
  <nav class="pagination">
    
      <a href="/posts/2019/08/29/engineering.html" class="pagination--pager" title="DS Interview Study Guide Part II: Software Engineering
">Previous</a>
    
    
      <a href="/post/2019/09/24/blogging-in-org.html" class="pagination--pager" title="Blogging in Org Mode
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 Ramya Bygari. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="http://localhost:4000/assets/js/main.min.js"></script>








<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<script src="https://use.fontawesome.com/releases/v5.0.6/js/all.js"></script>




  </body>
</html>
